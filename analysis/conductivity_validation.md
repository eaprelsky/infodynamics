# –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç–∏ (G_info)
## –ó–∞–¥–∞—á–∞ 4.1.2: –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞:** –Ø–Ω–≤–∞—Ä—å 2025  
**–°—Ç–∞—Ç—É—Å:** üî¨ –ê–ö–¢–ò–í–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø  
**–û—Å–Ω–æ–≤–∞:** theory/formal_model_conductivity.md + open data approach

---

## üéØ –¶–µ–ª—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏

–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç–∏:
```
G_info = k_ind √ó Relevance √ó (1 - Cognitive_Distance) √ó Attention_Focus √ó (1 - Cognitive_Load_Ratio)
```

### –ö–ª—é—á–µ–≤—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:
1. **H1:** G_info ‚àù Personal_Relevance (r > 0.4, p < 0.001)
2. **H2:** G_info ‚àù 1/Cognitive_Load (r < -0.4, p < 0.001)  
3. **H3:** G_info ‚àù Attention_Focus (r > 0.5, p < 0.001)
4. **H4:** Individual_differences (k_ind) –º–æ–¥–µ—Ä–∏—Ä—É—é—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã

---

## üìä Dataset 1: HCP Connectome Project

### –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:
```python
hcp_dataset_info = {
    "sample_size": 1206,  # Young adults (22-35 years)
    "demographics": "balanced_gender_education_ethnicity",
    "cognitive_measures": [
        "working_memory_tasks",  # N-back (0, 2-back)
        "attention_networks",    # Flanker task
        "processing_speed",      # Pattern comparison
        "episodic_memory",       # Picture sequence memory
        "executive_function",    # Dimensional change card sort
        "language_tasks"         # Reading, vocabulary
    ],
    "personality_data": [
        "big_five_personality",  # NEO-FFI
        "behavioral_measures",   # Delay discounting, etc.
        "self_regulation"        # Self-control measures
    ],
    "physiological": [
        "brain_imaging",         # fMRI, structural MRI
        "behavioral_timing",     # Reaction times, accuracy
        "individual_differences" # Comprehensive profiling
    ]
}
```

### Proxy measures –¥–ª—è G_info –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:
```python
hcp_proxy_measures = {
    "k_individual": {
        "source": "personality_cognitive_profile",
        "measures": [
            "openness_to_experience",  # NEO-FFI Openness
            "working_memory_capacity",  # N-back performance  
            "cognitive_flexibility",    # Card sort task
            "processing_speed_index"    # Pattern comparison speed
        ],
        "calculation": "composite_z_score_weighted_by_factor_loadings"
    },
    
    "attention_focus": {
        "source": "flanker_task_performance", 
        "measures": [
            "flanker_accuracy",         # Attention control
            "flanker_rt_consistency",   # Sustained attention
            "conflict_monitoring",      # Executive attention
            "alerting_efficiency"       # Attention networks
        ],
        "calculation": "attention_composite_score"
    },
    
    "cognitive_load_sensitivity": {
        "source": "n_back_task_performance",
        "measures": [
            "n_back_accuracy_decline",  # 2-back vs 0-back
            "working_memory_load_cost", # RT increase under load
            "error_rate_increase",      # Performance degradation
            "individual_load_threshold" # Personal capacity limits
        ],
        "calculation": "load_sensitivity_index"
    },
    
    "information_processing_efficiency": {
        "source": "multiple_cognitive_tasks",
        "measures": [
            "cross_task_consistency",   # G_info proxy
            "learning_transfer",        # Cognitive flexibility
            "task_switching_cost",      # Mental agility
            "composite_performance"     # Overall efficiency
        ],
        "calculation": "efficiency_factor_score"
    }
}
```

### Data preprocessing steps:
```python
def preprocess_hcp_data():
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ HCP –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ G_info –º–æ–¥–µ–ª–∏
    """
    
    # Step 1: Load and clean data
    hcp_data = load_hcp_behavioral_data()
    hcp_data = remove_outliers(hcp_data, method="modified_z_score", threshold=3.5)
    hcp_data = handle_missing_data(hcp_data, method="multiple_imputation")
    
    # Step 2: Calculate composite measures
    hcp_data['k_individual'] = calculate_individual_coefficient(
        openness=hcp_data['NEO_Openness'],
        wm_capacity=hcp_data['WM_Task_Acc'],
        processing_speed=hcp_data['ProcSpeed_AgeAdj'],
        cognitive_flexibility=hcp_data['CardSort_AgeAdj']
    )
    
    hcp_data['attention_focus'] = calculate_attention_composite(
        flanker_acc=hcp_data['Flanker_AgeAdj'],
        flanker_rt=hcp_data['Flanker_Unadj'],
        sustained_attention=hcp_data['SART_performance']
    )
    
    hcp_data['cognitive_load_ratio'] = calculate_load_sensitivity(
        nback_0_acc=hcp_data['WM_Task_0bk_Acc'],
        nback_2_acc=hcp_data['WM_Task_2bk_Acc'],
        nback_0_rt=hcp_data['WM_Task_0bk_RT'],
        nback_2_rt=hcp_data['WM_Task_2bk_RT']
    )
    
    # Step 3: Calculate G_info proxy
    hcp_data['g_info_proxy'] = calculate_conductivity_proxy(
        k_ind=hcp_data['k_individual'],
        attention=hcp_data['attention_focus'],
        load_ratio=hcp_data['cognitive_load_ratio']
    )
    
    return hcp_data

def calculate_individual_coefficient(openness, wm_capacity, processing_speed, cognitive_flexibility):
    """k_individual = weighted composite of personality and cognitive factors"""
    # Standardize all measures
    openness_z = zscore(openness)
    wm_z = zscore(wm_capacity) 
    speed_z = zscore(processing_speed)
    flexibility_z = zscore(cognitive_flexibility)
    
    # Weighted composite (weights from factor analysis)
    k_ind = (0.3 * openness_z + 0.25 * wm_z + 
             0.25 * speed_z + 0.2 * flexibility_z)
    
    return k_ind

def calculate_conductivity_proxy(k_ind, attention, load_ratio):
    """G_info proxy calculation"""
    # Note: We don't have relevance/cognitive_distance in HCP
    # So we test reduced model: G_info = k_ind √ó attention √ó (1 - load_ratio)
    g_proxy = k_ind * attention * (1 - load_ratio)
    return g_proxy
```

---

## üìä Dataset 2: MOOC Engagement Analysis

### –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:
```python
mooc_dataset_info = {
    "source": "harvard_mit_edx_courses",
    "sample_size": "100000+_students",
    "timespan": "2012_2019_longitudinal_data",
    "courses": [
        "computer_science",      # Programming, algorithms
        "mathematics",           # Statistics, calculus  
        "science",              # Physics, chemistry, biology
        "humanities",           # History, literature, philosophy
        "business"              # Economics, management
    ],
    "behavioral_measures": [
        "video_watching_patterns",    # Engagement proxy
        "quiz_performance",           # Learning efficiency
        "forum_participation",       # Social engagement
        "assignment_completion",     # Task persistence
        "course_completion_rate",    # Long-term commitment
        "time_spent_per_session"     # Attention duration
    ],
    "individual_differences": [
        "prior_education_level",     # Background knowledge
        "course_sequence_choices",   # Learning preferences
        "engagement_patterns",       # Behavioral signatures
        "performance_trajectories"   # Learning curves
    ]
}
```

### Proxy measures –¥–ª—è G_info validation:
```python
mooc_proxy_measures = {
    "personal_relevance": {
        "source": "course_selection_engagement_patterns",
        "measures": [
            "course_completion_rate",      # High relevance ‚Üí completion
            "time_spent_per_video",        # Engagement intensity
            "optional_content_consumption", # Interest-driven behavior
            "forum_question_quality"       # Deep engagement
        ],
        "calculation": "relevance_engagement_composite"
    },
    
    "cognitive_load_management": {
        "source": "learning_behavior_patterns",
        "measures": [
            "video_pause_rewind_frequency", # Load regulation
            "quiz_attempt_patterns",        # Difficulty management  
            "session_length_consistency",   # Load tolerance
            "content_skipping_behavior"     # Overload indicators
        ],
        "calculation": "load_management_index"
    },
    
    "information_conductivity": {
        "source": "learning_efficiency_measures",
        "measures": [
            "quiz_score_improvement_rate",  # Learning speed
            "concept_transfer_success",     # Information integration
            "knowledge_retention_decay",    # Memory persistence
            "cross_course_performance"      # Generalization
        ],
        "calculation": "learning_conductivity_score"
    }
}
```

### Analysis pipeline:
```python
def analyze_mooc_conductivity():
    """
    –ê–Ω–∞–ª–∏–∑ MOOC –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ G_info —á–µ—Ä–µ–∑ learning analytics
    """
    
    # Load MOOC data
    mooc_data = load_edx_behavioral_data()
    
    # Calculate relevance proxy
    mooc_data['relevance_proxy'] = calculate_relevance_from_engagement(
        completion_rate=mooc_data['course_completion'],
        time_per_video=mooc_data['video_watch_time'],
        optional_consumption=mooc_data['optional_content_viewed'],
        forum_quality=mooc_data['forum_engagement_quality']
    )
    
    # Calculate cognitive load proxy  
    mooc_data['cognitive_load_proxy'] = calculate_load_from_behavior(
        pause_frequency=mooc_data['video_pause_count'],
        quiz_attempts=mooc_data['quiz_retry_patterns'],
        session_consistency=mooc_data['study_session_regularity'],
        content_skipping=mooc_data['content_skip_rate']
    )
    
    # Calculate conductivity outcome
    mooc_data['conductivity_outcome'] = calculate_learning_efficiency(
        improvement_rate=mooc_data['quiz_score_trajectory'],
        transfer_success=mooc_data['cross_module_performance'],
        retention=mooc_data['knowledge_retention_test'],
        generalization=mooc_data['new_concept_application']
    )
    
    # Test G_info predictions
    test_conductivity_predictions(mooc_data)
    
    return mooc_data

def calculate_relevance_from_engagement(completion_rate, time_per_video, optional_consumption, forum_quality):
    """Personal relevance proxy from engagement behaviors"""
    # High relevance should predict higher engagement across metrics
    relevance_proxy = (
        0.4 * zscore(completion_rate) +
        0.3 * zscore(time_per_video) + 
        0.2 * zscore(optional_consumption) +
        0.1 * zscore(forum_quality)
    )
    return relevance_proxy

def test_conductivity_predictions(data):
    """Test core G_info predictions on MOOC data"""
    
    # H1: G_info ‚àù Personal_Relevance
    correlation_relevance = pearsonr(data['relevance_proxy'], data['conductivity_outcome'])
    
    # H2: G_info ‚àù 1/Cognitive_Load  
    correlation_load = pearsonr(data['cognitive_load_proxy'], data['conductivity_outcome'])
    
    # Regression model
    model = smf.ols('conductivity_outcome ~ relevance_proxy + I(1/cognitive_load_proxy) + relevance_proxy:I(1/cognitive_load_proxy)', data=data).fit()
    
    return {
        'relevance_correlation': correlation_relevance,
        'load_correlation': correlation_load,
        'regression_model': model
    }
```

---

## üìä Dataset 3: Social Media Information Cascades

### –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:
```python
social_media_dataset = {
    "source": "twitter_information_cascades",
    "sample_size": "millions_of_tweets_thousands_of_users",
    "timespan": "2015_2020_longitudinal_tracking",
    "content_types": [
        "breaking_news",         # High urgency, relevance
        "scientific_findings",   # Complex, low emotional charge
        "political_opinions",    # High emotional charge
        "entertainment_content", # High engagement, low complexity
        "educational_content"    # Medium complexity, variable relevance
    ],
    "user_measures": [
        "follower_count",        # Social influence
        "posting_frequency",     # Activity level
        "engagement_patterns",   # Interaction behaviors
        "topic_expertise",       # Domain knowledge indicators
        "network_position"       # Social network centrality
    ],
    "cascade_measures": [
        "retweet_speed",         # Information flow velocity
        "cascade_depth",         # Information penetration
        "user_engagement_rate",  # Individual conductivity
        "content_modification",  # Information transformation
        "cascade_persistence"    # Temporal dynamics
    ]
}
```

### G_info validation approach:
```python
def validate_social_conductivity():
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è G_info –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π
    """
    
    # Load Twitter cascade data
    twitter_data = load_twitter_cascade_dataset()
    
    # Calculate user-level conductivity proxies
    twitter_data['user_k_individual'] = calculate_user_individual_factor(
        follower_count=twitter_data['followers'],
        posting_frequency=twitter_data['posts_per_day'],
        engagement_history=twitter_data['avg_engagement_received'],
        topic_expertise=twitter_data['domain_authority_score']
    )
    
    # Calculate content-level factors
    twitter_data['content_relevance'] = calculate_content_relevance(
        user_topic_match=twitter_data['user_topic_alignment'],
        trending_status=twitter_data['trending_score'],
        personal_mentions=twitter_data['mentions_user_interests']
    )
    
    twitter_data['emotional_charge'] = calculate_emotional_content(
        sentiment_intensity=twitter_data['sentiment_score'],
        emotional_language=twitter_data['emotion_word_count'],
        urgency_markers=twitter_data['urgency_indicators']
    )
    
    # Calculate information conductivity outcome
    twitter_data['cascade_conductivity'] = calculate_cascade_efficiency(
        retweet_speed=twitter_data['time_to_first_100_retweets'],
        penetration_depth=twitter_data['cascade_max_depth'],
        engagement_rate=twitter_data['retweet_like_ratio'],
        persistence=twitter_data['cascade_half_life']
    )
    
    # Test G_info model predictions
    validate_cascade_predictions(twitter_data)
    
    return twitter_data

def calculate_cascade_efficiency(retweet_speed, penetration_depth, engagement_rate, persistence):
    """Calculate information conductivity from cascade dynamics"""
    # Faster spread + deeper penetration + higher engagement = higher conductivity
    speed_norm = 1 / (1 + retweet_speed)  # Faster = higher conductivity
    depth_norm = zscore(penetration_depth)
    engagement_norm = zscore(engagement_rate)  
    persistence_norm = zscore(persistence)
    
    conductivity = (0.4 * speed_norm + 0.3 * depth_norm + 
                   0.2 * engagement_norm + 0.1 * persistence_norm)
    
    return conductivity

def validate_cascade_predictions(data):
    """Test G_info predictions on information cascade data"""
    
    # Test individual differences effect
    individual_effect = pearsonr(data['user_k_individual'], data['cascade_conductivity'])
    
    # Test content relevance effect
    relevance_effect = pearsonr(data['content_relevance'], data['cascade_conductivity'])
    
    # Test emotional charge effect (threshold model)
    emotional_groups = pd.cut(data['emotional_charge'], bins=3, labels=['low', 'medium', 'high'])
    emotional_anova = stats.f_oneway(*[data[emotional_groups == group]['cascade_conductivity'] 
                                     for group in ['low', 'medium', 'high']])
    
    # Comprehensive regression model
    model = smf.ols('''cascade_conductivity ~ 
                      user_k_individual + 
                      content_relevance + 
                      emotional_charge + 
                      I(emotional_charge**2) +
                      user_k_individual:content_relevance +
                      user_k_individual:emotional_charge''', 
                   data=data).fit()
    
    return {
        'individual_correlation': individual_effect,
        'relevance_correlation': relevance_effect,
        'emotional_anova': emotional_anova,
        'full_model': model
    }
```

---

## üìà –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### Cross-dataset validation:
```python
def conduct_meta_analysis():
    """
    Meta-analysis across all three datasets
    """
    
    # Results from each dataset
    hcp_results = analyze_hcp_conductivity()
    mooc_results = analyze_mooc_conductivity() 
    twitter_results = validate_social_conductivity()
    
    # Extract effect sizes
    effect_sizes = {
        'individual_differences': [
            hcp_results['k_individual_effect'],
            mooc_results['user_profile_effect'],
            twitter_results['individual_correlation'][0]
        ],
        'relevance_effects': [
            None,  # Not available in HCP
            mooc_results['relevance_correlation'][0],
            twitter_results['relevance_correlation'][0]  
        ],
        'cognitive_load_effects': [
            hcp_results['load_correlation'][0],
            mooc_results['load_correlation'][0], 
            None   # Not directly measured in Twitter
        ]
    }
    
    # Meta-analysis for each effect
    meta_results = {}
    for effect_name, correlations in effect_sizes.items():
        if any(corr is not None for corr in correlations):
            valid_correlations = [r for r in correlations if r is not None]
            meta_results[effect_name] = {
                'mean_effect': np.mean(valid_correlations),
                'confidence_interval': stats.t.interval(0.95, len(valid_correlations)-1, 
                                                       loc=np.mean(valid_correlations),
                                                       scale=stats.sem(valid_correlations)),
                'heterogeneity': np.std(valid_correlations)
            }
    
    return meta_results

def generate_validation_report():
    """
    Comprehensive validation report
    """
    
    meta_results = conduct_meta_analysis()
    
    report = {
        'model_validation_summary': {
            'h1_individual_differences': {
                'prediction': 'k_individual moderates information conductivity',
                'evidence': f"Mean r = {meta_results['individual_differences']['mean_effect']:.3f}",
                'support': 'STRONG' if abs(meta_results['individual_differences']['mean_effect']) > 0.3 else 'MODERATE'
            },
            'h2_relevance_effect': {
                'prediction': 'G_info ‚àù Personal_Relevance',
                'evidence': f"Mean r = {meta_results['relevance_effects']['mean_effect']:.3f}",
                'support': 'STRONG' if meta_results['relevance_effects']['mean_effect'] > 0.4 else 'MODERATE'
            },
            'h3_cognitive_load': {
                'prediction': 'G_info ‚àù 1/Cognitive_Load',
                'evidence': f"Mean r = {meta_results['cognitive_load_effects']['mean_effect']:.3f}",
                'support': 'STRONG' if meta_results['cognitive_load_effects']['mean_effect'] < -0.3 else 'MODERATE'
            }
        },
        'practical_applications': {
            'education': 'Adaptive content difficulty based on individual k_ind',
            'social_media': 'Predict viral potential from content-user match',
            'interface_design': 'Optimize information density for cognitive load'
        },
        'model_refinements': {
            'parameter_estimates': 'Data-driven coefficients for G_info formula',
            'boundary_conditions': 'Contexts where model most/least applicable',
            'individual_profiles': 'Conductivity clusters for personalization'
        }
    }
    
    return report
```

---

## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—à–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏:
```python
validation_success_criteria = {
    'statistical_significance': {
        'correlation_strength': 'r > 0.3 for major predictions',
        'consistency_across_datasets': 'same_direction_effects_in_2+_datasets',
        'effect_size': 'cohen_d > 0.3_for_practical_relevance'
    },
    'theoretical_consistency': {
        'individual_differences': 'k_individual_significantly_moderates_conductivity',
        'cognitive_load': 'negative_relationship_with_conductivity_confirmed',
        'relevance': 'positive_relationship_with_conductivity_confirmed'
    },
    'practical_utility': {
        'prediction_accuracy': 'model_explains_>25%_variance_in_outcomes',
        'generalization': 'effects_robust_across_cognitive_social_educational_domains',
        'application_potential': 'clear_guidelines_for_real_world_implementation'
    }
}
```

### –ù–∞—É—á–Ω—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:
1. **–ü–µ—Ä–≤–∞—è —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è** —Ç–µ–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏
2. **Data-driven –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è G_info –º–æ–¥–µ–ª–∏  
3. **Cross-domain evidence** –¥–ª—è universal applicability
4. **Reproducible methodology** –¥–ª—è future research

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ó–ê–î–ê–ß–ê 4.1.2 –ì–û–¢–û–í–ê –ö –í–´–ü–û–õ–ù–ï–ù–ò–Æ**

**–ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:**
1. **Week 1-2:** Data collection –∏ preprocessing (HCP, MOOC, Twitter)
2. **Week 3-4:** Individual dataset analysis –∏ validation
3. **Week 5-6:** Cross-dataset meta-analysis –∏ integrated modeling
4. **Week 7:** Validation report –∏ practical applications

**–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å:** üöÄ **–ù–ï–ú–ï–î–õ–ï–ù–ù–û–ï –í–´–ü–û–õ–ù–ï–ù–ò–ï** 